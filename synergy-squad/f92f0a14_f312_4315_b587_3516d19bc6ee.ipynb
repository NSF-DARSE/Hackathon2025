{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1: Query a table with SQL and magic commands"
      ],
      "metadata": {
        "id": "VI0xLxmm7HBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running this code will query a table in BigQuery and download\n",
        "# the results to a Pandas DataFrame named `results`.\n",
        "# Learn more here: https://cloud.google.com/bigquery/docs/visualize-jupyter\n",
        "\n",
        "%%bigquery results --project jpmc2-468719\n",
        "SELECT * FROM `jpmc2-468719.Delaware_Checkbook_Expenditure.Expenditure` #this table name was set based on the table you chose to query"
      ],
      "metadata": {
        "id": "7apwrP5jxUGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can view the resulting Pandas DataFrame and work with using the Pandas library.\n",
        "# https://pandas.pydata.org/docs/getting_started/index.html#getting-started\n",
        "results"
      ],
      "metadata": {
        "id": "itFJzBFNr72l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Query a table with BigQuery DataFrames"
      ],
      "metadata": {
        "id": "PYt5QGSf67N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With BigQuery DataFrames, you can use many familiar Pandas methods, but the\n",
        "# processing happens in BigQuery rather than the runtime, allowing you to work with larger\n",
        "# DataFrames that would otherwise not fit in the runtime memory.\n",
        "# Learn more here: https://cloud.google.com/python/docs/reference/bigframes/latest\n",
        "\n",
        "import bigframes.pandas as bf\n",
        "\n",
        "bf.options.bigquery.location = \"us-east1\" #this variable is set based on the dataset you chose to query\n",
        "bf.options.bigquery.project = \"jpmc2-468719\" #this variable is set based on the dataset you chose to query"
      ],
      "metadata": {
        "id": "tJSfAiHycbAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = bf.read_gbq(\"jpmc2-468719.Delaware_Checkbook_Expenditure.Expenditure\") #this variable is set based on the dataset you chose to query"
      ],
      "metadata": {
        "id": "fUsQzlgjiA_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BigFrames can work with tables that are too large to fit in the notebook memory.\n",
        "# Look at the first 20 rows.\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "YQguJBV6iRSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#  DATA LOADING & PREPROCESSING\n",
        "def load_and_preprocess_data_fast(file_path, sample_size=None):\n",
        "    \"\"\"\n",
        "    Fast data loading with optional sampling for large datasets\n",
        "    \"\"\"\n",
        "    print(\"Loading dataset...\")\n",
        "    # Load with efficient dtypes\n",
        "    df = pd.read_csv(file_path, dtype={\n",
        "        'Department': 'category',\n",
        "        'Vendor': 'category',\n",
        "        'Category': 'category'\n",
        "    })\n",
        "    # Sample if dataset is too large\n",
        "    if sample_size and len(df) > sample_size:\n",
        "        print(f\" Sampling {sample_size:,} rows from {len(df):,} total rows\")\n",
        "        df = df.sample(n=sample_size, random_state=42)\n",
        "    # Auto-detect column mappings\n",
        "    column_mapping = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'department' in col_lower or 'agency' in col_lower:\n",
        "            column_mapping[col] = 'department'\n",
        "        elif 'vendor' in col_lower or 'supplier' in col_lower:\n",
        "            column_mapping[col] = 'vendor'\n",
        "        elif 'amount' in col_lower or 'total' in col_lower or 'payment' in col_lower:\n",
        "            column_mapping[col] = 'amount'\n",
        "        elif 'category' in col_lower or 'type' in col_lower:\n",
        "            column_mapping[col] = 'category'\n",
        "        elif 'description' in col_lower or 'item' in col_lower:\n",
        "            column_mapping[col] = 'description'\n",
        "    df = df.rename(columns=column_mapping)\n",
        "    # Clean data efficiently\n",
        "    if 'amount' in df.columns:\n",
        "        df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
        "        df = df.dropna(subset=['amount'])\n",
        "        df = df[df['amount'] > 0]\n",
        "    # Clean text columns\n",
        "    text_cols = ['department', 'vendor', 'category', 'description']\n",
        "    for col in text_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip().str.upper()\n",
        "    print(f\" Loaded {len(df):,} records\")\n",
        "    print(f\" Columns: {list(df.columns)}\")\n",
        "    print(f\" Total spending: ${df['amount'].sum():,.2f}\")\n",
        "    return df\n",
        "#  PRICE VARIANCE ANALYSIS\n",
        "def fast_price_variance_analysis(df, max_categories=20):\n",
        "    \"\"\"\n",
        "    Optimized price variance analysis without fuzzy matching\n",
        "    \"\"\"\n",
        "    print(\" Starting fast price variance analysis...\")\n",
        "    results = []\n",
        "    # Focus on categories with sufficient data\n",
        "    if 'category' in df.columns:\n",
        "        category_counts = df['category'].value_counts()\n",
        "        top_categories = category_counts.head(max_categories).index\n",
        "        for category in top_categories:\n",
        "            category_data = df[df['category'] == category]\n",
        "            if len(category_data) < 5:  # Skip categories with too few records\n",
        "                continue\n",
        "# Use vectorized operations\n",
        "            price_stats = {\n",
        "                'item_name': f\"Category: {category}\",\n",
        "                'num_purchases': len(category_data),\n",
        "                'mean_price': category_data['amount'].mean(),\n",
        "                'median_price': category_data['amount'].median(),\n",
        "                'std_price': category_data['amount'].std(),\n",
        "                'min_price': category_data['amount'].min(),\n",
        "                'max_price': category_data['amount'].max(),\n",
        "                'total_spent': category_data['amount'].sum()\n",
        "            }\n",
        "            # Calculate variance metrics\n",
        "            price_stats['coefficient_of_variation'] = (\n",
        "                price_stats['std_price'] / price_stats['mean_price']\n",
        "                if price_stats['mean_price'] > 0 else 0\n",
        "            )\n",
        "            # Potential savings calculation\n",
        "            price_stats['potential_savings'] = (\n",
        "                price_stats['total_spent'] -\n",
        "                (price_stats['num_purchases'] * price_stats['min_price'])\n",
        "            )\n",
        "            price_stats['savings_percentage'] = (\n",
        "                price_stats['potential_savings'] / price_stats['total_spent'] * 100\n",
        "                if price_stats['total_spent'] > 0 else 0\n",
        "            )\n",
        "            # Add department/vendor info\n",
        "            if 'department' in df.columns:\n",
        "                price_stats['num_departments'] = category_data['department'].nunique()\n",
        "            if 'vendor' in df.columns:\n",
        "                price_stats['num_vendors'] = category_data['vendor'].nunique()\n",
        "            # Priority classification\n",
        "            if price_stats['potential_savings'] > 50000:\n",
        "                price_stats['priority'] = 'CRITICAL'\n",
        "            elif price_stats['potential_savings'] > 10000:\n",
        "                price_stats['priority'] = 'HIGH'\n",
        "            elif price_stats['potential_savings'] > 5000:\n",
        "                price_stats['priority'] = 'MEDIUM'\n",
        "            else:\n",
        "                price_stats['priority'] = 'LOW'\n",
        "            # Only include items with significant variation\n",
        "            if price_stats['coefficient_of_variation'] > 0.2:\n",
        "                results.append(price_stats)\n",
        "# Department-Vendor analysis (vectorized)\n",
        "    if 'department' in df.columns and 'vendor' in df.columns:\n",
        "        dept_vendor_stats = df.groupby(['department', 'vendor']).agg({\n",
        "            'amount': ['count', 'mean', 'std', 'sum']\n",
        "        }).reset_index()\n",
        "        # Flatten column names\n",
        "        dept_vendor_stats.columns = [\n",
        "            'department', 'vendor', 'count', 'mean_amount', 'std_amount', 'total_amount'\n",
        "        ]\n",
        "        # Find high-variation department-vendor pairs\n",
        "        dept_vendor_stats['cv'] = dept_vendor_stats['std_amount'] / dept_vendor_stats['mean_amount']\n",
        "        high_variation = dept_vendor_stats[\n",
        "            (dept_vendor_stats['cv'] > 0.3) &\n",
        "            (dept_vendor_stats['count'] > 3) &\n",
        "            (dept_vendor_stats['total_amount'] > 5000)\n",
        "        ]\n",
        "        for _, row in high_variation.iterrows():\n",
        "            results.append({\n",
        "                'item_name': f\"Dept: {row['department']} | Vendor: {row['vendor']}\",\n",
        "                'num_purchases': row['count'],\n",
        "                'mean_price': row['mean_amount'],\n",
        "                'std_price': row['std_amount'],\n",
        "                'coefficient_of_variation': row['cv'],\n",
        "                'total_spent': row['total_amount'],\n",
        "                'potential_savings': row['total_amount'] * 0.1,  # Estimate 10% savings\n",
        "                'priority': 'HIGH' if row['total_amount'] > 25000 else 'MEDIUM'\n",
        "            })\n",
        "    return sorted(results, key=lambda x: x.get('potential_savings', 0), reverse=True)\n",
        "def fast_clustering_analysis(df, sample_size=10000):\n",
        "    \"\"\"\n",
        "    Fast clustering with sampling for large datasets\n",
        "    \"\"\"\n",
        "    print(\" Starting fast clustering analysis...\")\n",
        "    # Sample for clustering if dataset is large\n",
        "    if len(df) > sample_size:\n",
        "        print(f\"Sampling {sample_size:,} rows for clustering\")\n",
        "        cluster_df = df.sample(n=sample_size, random_state=42)\n",
        "    else:\n",
        "        cluster_df = df.copy()\n",
        "    # Prepare features efficiently\n",
        "    features = pd.DataFrame()\n",
        "    features['amount'] = cluster_df['amount']\n",
        "    features['log_amount'] = np.log1p(cluster_df['amount'])\n",
        "    # Encode categoricals efficiently\n",
        "    le_dict = {}\n",
        "    for col in ['department', 'vendor', 'category']:\n",
        "        if col in cluster_df.columns:\n",
        "            le = LabelEncoder()\n",
        "            features[f'{col}_encoded'] = le.fit_transform(cluster_df[col].astype(str))\n",
        "            le_dict[col] = le\n",
        "    # Add frequency features\n",
        "    if 'vendor' in cluster_df.columns:\n",
        "        vendor_counts = cluster_df['vendor'].value_counts()\n",
        "        features['vendor_frequency'] = cluster_df['vendor'].map(vendor_counts)\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features.fillna(0))\n",
        "    # Fast K-means with fewer clusters\n",
        "    n_clusters = min(8, len(features) // 100)  # Adaptive cluster count\n",
        "    if n_clusters >= 2:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=5)\n",
        "        cluster_labels = kmeans.fit_predict(features_scaled)\n",
        "        # Analyze clusters\n",
        "        cluster_df['cluster'] = cluster_labels\n",
        "        cluster_summary = []\n",
        "        for cluster_id in range(n_clusters):\n",
        "            cluster_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
        "            summary = {\n",
        "                'cluster_id': cluster_id,\n",
        "                'size': len(cluster_data),\n",
        "                'percentage': len(cluster_data) / len(cluster_df) * 100,\n",
        "                'avg_amount': cluster_data['amount'].mean(),\n",
        "                'median_amount': cluster_data['amount'].median(),\n",
        "                'total_spending': cluster_data['amount'].sum()\n",
        "            }\n",
        "            if 'department' in cluster_data.columns:\n",
        "                summary['top_department'] = cluster_data['department'].mode().iloc[0]\n",
        "                summary['num_departments'] = cluster_data['department'].nunique()\n",
        "            cluster_summary.append(summary)\n",
        "        return {\n",
        "            'cluster_labels': cluster_labels,\n",
        "            'n_clusters': n_clusters,\n",
        "            'cluster_analysis': cluster_summary,\n",
        "            'features_used': list(features.columns)\n",
        "        }\n",
        "    return None\n",
        "# FAST VISUALIZATION\n",
        "def create_fast_visualizations(price_results, clustering_results=None):\n",
        "    \"\"\"\n",
        "    Create quick visualizations of results\n",
        "    \"\"\"\n",
        "    print(\" Creating visualizations...\")\n",
        "    if price_results:\n",
        "        # Top opportunities chart\n",
        "        top_10 = price_results[:10]\n",
        "        if top_10:\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            # Savings potential\n",
        "            items = [r['item_name'][:30] + '...' if len(r['item_name']) > 30\n",
        "                    else r['item_name'] for r in top_10]\n",
        "            savings = [r.get('potential_savings', 0) for r in top_10]\n",
        "            ax1.barh(items, savings)\n",
        "            ax1.set_xlabel('Potential Savings ($)')\n",
        "            ax1.set_title('Top 10 Consolidation Opportunities')\n",
        "            ax1.tick_params(axis='y', labelsize=8)\n",
        "            # Price variation\n",
        "            cv_values = [r.get('coefficient_of_variation', 0) * 100 for r in top_10]\n",
        "            ax2.barh(items, cv_values)\n",
        "            ax2.set_xlabel('Price Variation (%)')\n",
        "            ax2.set_title('Price Variation by Category')\n",
        "            ax2.tick_params(axis='y', labelsize=8)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    if clustering_results:\n",
        "        # Cluster distribution\n",
        "        cluster_analysis = clustering_results.get('cluster_analysis', [])\n",
        "        if cluster_analysis:\n",
        "            sizes = [c['size'] for c in cluster_analysis]\n",
        "            labels = [f\"Cluster {c['cluster_id']}\" for c in cluster_analysis]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "            plt.title('Spending Pattern Clusters')\n",
        "            plt.show()\n",
        "#  FAST ANALYSIS FUNCTION\n",
        "def run_fast_analysis(file_path, sample_size=50000):\n",
        "    \"\"\"\n",
        "    Run optimized analysis for Colab\n",
        "    \"\"\"\n",
        "    print(\"DELAWARE EXPENDITURE ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    # Load data with sampling\n",
        "    df = load_and_preprocess_data_fast(file_path, sample_size=sample_size)\n",
        "    # Fast price analysis\n",
        "    print(\"\\n Running price variance analysis...\")\n",
        "    price_results = fast_price_variance_analysis(df, max_categories=15)\n",
        "    print(f\"\\n Found {len(price_results)} consolidation opportunities\")\n",
        "    if price_results:\n",
        "        total_savings = sum(r.get('potential_savings', 0) for r in price_results)\n",
        "        print(f\":moneybag: Total potential savings: ${total_savings:,.2f}\")\n",
        "        print(\"\\n TOP 5 OPPORTUNITIES:\")\n",
        "        for i, result in enumerate(price_results[:5], 1):\n",
        "            print(f\"{i}. {result['item_name'][:50]}\")\n",
        "            print(f\"    Savings: ${result.get('potential_savings', 0):,.2f}\")\n",
        "            print(f\"    Variation: {result.get('coefficient_of_variation', 0):.1%}\")\n",
        "    # Fast clustering\n",
        "    print(\"\\n Running clustering analysis...\")\n",
        "    clustering_results = fast_clustering_analysis(df, sample_size=10000)\n",
        "    if clustering_results:\n",
        "        print(f\" Identified {clustering_results['n_clusters']} spending patterns\")\n",
        "    # Quick visualizations\n",
        "    create_fast_visualizations(price_results, clustering_results)\n",
        "    return {\n",
        "        'price_results': price_results,\n",
        "        'clustering_results': clustering_results,\n",
        "        'dataset_info': {\n",
        "            'total_records': len(df),\n",
        "            'total_spending': df['amount'].sum(),\n",
        "            'departments': df['department'].nunique() if 'department' in df.columns else 0,\n",
        "            'vendors': df['vendor'].nunique() if 'vendor' in df.columns else 0\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage in Colab\n",
        "    FILE_PATH = \"Delaware_Checkbook_Expenditure_Details_20250813.csv\"  # Your file path\n",
        "    # Run fast analysis (samples large datasets automatically)\n",
        "    results = run_fast_analysis(FILE_PATH, sample_size=50000)\n",
        "    print(\"\\n Analysis complete!\")\n",
        "    print(\" Key optimizations applied:\")\n",
        "    print(\"   • Removed slow fuzzy matching\")\n",
        "    print(\"   • Used vectorized pandas operations\")\n",
        "    print(\"   • Sampled large datasets\")\n",
        "    print(\"   • Focused on top categories only\")\n",
        "    print(\"   • Efficient clustering with fewer iterations\")\n"
      ],
      "metadata": {
        "id": "p2NN8wwF7YRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from BigQuery.\n",
        "table = spark.read.format('bigquery') \\\n",
        "  .option('table', 'jpmc2-468719.Delaware_Checkbook_Expenditure.Expenditure') \\\n",
        "  .load()\n",
        "table.createOrReplaceTempView('table')\n",
        "\n",
        "# Explore the data and schema.\n",
        "table_data = spark.sql(\n",
        "    'SELECT * FROM penguins')\n",
        "table_data.show()\n",
        "table_data.printSchema()"
      ],
      "metadata": {
        "id": "9AyZuQMpZE92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}